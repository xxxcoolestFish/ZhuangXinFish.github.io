<!DOCTYPE html>


<html lang="zh">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>新闻文本分类项目：从TextCNN到多粒度BERT模型的演进 |  很荣幸被您认识</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-新闻文本分类项目：从TextCNN到多粒度BERT模型的演进"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  新闻文本分类项目：从TextCNN到多粒度BERT模型的演进
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/03/28/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE%EF%BC%9A%E4%BB%8ETextCNN%E5%88%B0%E5%A4%9A%E7%B2%92%E5%BA%A6BERT%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E8%BF%9B/" class="article-date">
  <time datetime="2025-03-28T03:03:09.000Z" itemprop="datePublished">2025-03-28</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E9%A1%B9%E7%9B%AE/">项目</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">4k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">18 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1>新闻文本分类实践记录</h1>
<p>本文主要分享新闻文本分类的两个实践版本：第一版基于TextCNN，通过jieba分词、自建词表和固定长度序列处理数据，利用多尺度卷积提取特征，还提及预训练词向量加载、数据清洗等优化方向；第二版采用Multi-Granularity BERT增强模型，融合BERT不同层次隐藏状态，结合多尺度卷积和自注意力机制强化语义理解，通过分层学习率等策略提升效果。此外，文章还介绍了避免浏览器渲染Markdown出错的方法，如包裹代码块、转义特殊符号等，最后总结两版特点——TextCNN适合快速搭建基线，BERT版适合追求高精度场景，并强调工程实践中的细节处理。</p>
<span id="more"></span>
<hr>
<h2 id="版本一：textcnn-实现">版本一：TextCNN 实现</h2>
<p>其实我最开始并没有直接上大模型，而是想先用一个简单点的方法跑通全流程。思路就是这样：先把新闻读进来，用 <code>jieba</code> 分词，然后自己造个词表，把每条新闻切成固定长度的词索引序列，再用一个 TextCNN 模型做分类。下面把核心代码贴出来，接着我会在段落里详细说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextClassifierDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path, vocab=<span class="literal">None</span>, label_map=<span class="literal">None</span>, max_len=<span class="number">100</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.labels, <span class="variable language_">self</span>.texts = [], []</span><br><span class="line">        label_counter = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1. 读取文件并分词</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                label, text = line.strip().split(<span class="string">&#x27;\t&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">                <span class="variable language_">self</span>.labels.append(label)</span><br><span class="line">                <span class="variable language_">self</span>.texts.append(<span class="built_in">list</span>(jieba.cut(text)))</span><br><span class="line">                label_counter[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 标签映射：训练时自己生成，测试时沿用</span></span><br><span class="line">        <span class="keyword">if</span> label_map <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.label_map = &#123;label: idx <span class="keyword">for</span> idx, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_counter.keys())&#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.label_map = label_map</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 3. 词表构建：统计所有分词，把高频前20000个词拿出来</span></span><br><span class="line">        <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            word_counts = Counter([word <span class="keyword">for</span> text <span class="keyword">in</span> <span class="variable language_">self</span>.texts <span class="keyword">for</span> word <span class="keyword">in</span> text])</span><br><span class="line">            <span class="variable language_">self</span>.vocab = &#123;</span><br><span class="line">                <span class="string">&#x27;&lt;PAD&gt;&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;&lt;UNK&gt;&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">                **&#123;word: i+<span class="number">2</span> <span class="keyword">for</span> i, (word, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_counts.most_common(<span class="number">20000</span>))&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.vocab = vocab</span><br><span class="line">            </span><br><span class="line">        <span class="variable language_">self</span>.max_len = max_len</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.texts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        text = <span class="variable language_">self</span>.texts[idx][:<span class="variable language_">self</span>.max_len]</span><br><span class="line">        <span class="comment"># 把词映射到索引，没在词表里的用1（&lt;UNK&gt;），再pad到max_len</span></span><br><span class="line">        text_indices = [<span class="variable language_">self</span>.vocab.get(word, <span class="number">1</span>) <span class="keyword">for</span> word <span class="keyword">in</span> text]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(text_indices) &lt; <span class="variable language_">self</span>.max_len:</span><br><span class="line">            text_indices += [<span class="number">0</span>] * (<span class="variable language_">self</span>.max_len - <span class="built_in">len</span>(text_indices))</span><br><span class="line">        </span><br><span class="line">        label = <span class="variable language_">self</span>.label_map[<span class="variable language_">self</span>.labels[idx]]</span><br><span class="line">        <span class="keyword">return</span> torch.LongTensor(text_indices), torch.tensor(label, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_dim, num_classes, filter_sizes=(<span class="params"><span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span>), num_filters=<span class="number">100</span>, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        <span class="comment"># 多尺寸卷积</span></span><br><span class="line">        <span class="variable language_">self</span>.convs = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, num_filters, (k, embed_dim))</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> filter_sizes</span><br><span class="line">        ])</span><br><span class="line">        <span class="comment"># 分类头</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(num_filters * <span class="built_in">len</span>(filter_sizes), num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: [batch, seq_len]</span></span><br><span class="line">        x = <span class="variable language_">self</span>.embedding(x)         <span class="comment"># [batch, seq_len, embed_dim]</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>)            <span class="comment"># [batch, 1, seq_len, embed_dim]</span></span><br><span class="line">        </span><br><span class="line">        pooled_outputs = []</span><br><span class="line">        <span class="keyword">for</span> conv <span class="keyword">in</span> <span class="variable language_">self</span>.convs:</span><br><span class="line">            conv_out = torch.relu(conv(x)).squeeze(<span class="number">3</span>)  <span class="comment"># [batch, num_filters, seq_len - k + 1]</span></span><br><span class="line">            pooled = torch.<span class="built_in">max</span>(conv_out, dim=<span class="number">2</span>)[<span class="number">0</span>]     <span class="comment"># [batch, num_filters]</span></span><br><span class="line">            pooled_outputs.append(pooled)</span><br><span class="line">            </span><br><span class="line">        x = torch.cat(pooled_outputs, dim=<span class="number">1</span>)          <span class="comment"># [batch, num_filters*len(filter_sizes)]</span></span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)                             <span class="comment"># [batch, num_classes]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>():</span><br><span class="line">    BATCH_SIZE = <span class="number">64</span></span><br><span class="line">    EMBED_DIM = <span class="number">300</span></span><br><span class="line">    EPOCHS = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    train_set = TextClassifierDataset(<span class="string">&#x27;train.txt&#x27;</span>)</span><br><span class="line">    test_set = TextClassifierDataset(<span class="string">&#x27;test.txt&#x27;</span>, </span><br><span class="line">                                     vocab=train_set.vocab,</span><br><span class="line">                                     label_map=train_set.label_map)</span><br><span class="line">    </span><br><span class="line">    model = TextCNN(</span><br><span class="line">        vocab_size=<span class="built_in">len</span>(train_set.vocab),</span><br><span class="line">        embed_dim=EMBED_DIM,</span><br><span class="line">        num_classes=<span class="built_in">len</span>(train_set.label_map)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    model.to(device)</span><br><span class="line">    </span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">            inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        </span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">                inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">                outputs = model(inputs)</span><br><span class="line">                preds = torch.argmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line">                correct += (preds == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">                </span><br><span class="line">        accuracy = correct / <span class="built_in">len</span>(test_set)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;EPOCHS&#125;</span> | Test Acc: <span class="subst">&#123;accuracy:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_model()</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">看完这段代码，思路其实挺直白。先把新闻文本按行读进来，用 `jieba` 切成一个个词，然后把每个词映射到词表索引（不存在的用 `&lt;UNK&gt;`），固定长度截断/补齐，得到一个 `[max_len]` 的整数列表，给模型做 Embedding 时就能直接拿整数索引了。嵌入层之后，TextCNN 核心在于多尺寸的 2D 卷积：把 `[batch, <span class="number">1</span>, seq_len, embed_dim]` 看成一个“单通道图像”，三个卷积核大小分别是 `(<span class="number">3</span>, embed_dim)`、`(<span class="number">4</span>, embed_dim)`、`(<span class="number">5</span>, embed_dim)`，也就是在时间方向上滑三格、四格、五格，每次都覆盖整个 `embed_dim`。卷积后做 ReLU，再对序列方向做 <span class="built_in">max</span>-over-time pooling，就剩下 `[batch, num_filters]`，三个尺寸的输出拼在一起，Dropout 之后接全连接层拿 logits，用交叉熵损失训练。每个 epoch 跑完训练，就在测试集上 <span class="built_in">eval</span> 一下，算个整体准确率。</span><br><span class="line"></span><br><span class="line">**几点心得和可改进之处：**</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **预训练词向量。**</span><br><span class="line">   代码里嵌入层随机初始化，如果想让模型更快收敛或效果更好，可以把中文的预训练词向量（比如 Tencent AI Lab 或 FastText）加载进来：</span><br><span class="line"></span><br><span class="line">   ```Python</span><br><span class="line">   embed_matrix = load_pretrained_embeddings()  <span class="comment"># 形状 [vocab_size, EMBED_DIM]</span></span><br><span class="line">   model.embedding.weight.data.copy_(torch.from_numpy(embed_matrix))</span><br></pre></td></tr></table></figure>
<p>如果不想在训练时破坏预训练向量，还可以设置 <code>model.embedding.weight.requires_grad = False</code>，固定这部分权重。</p>
<ol start="2">
<li>
<p><strong>监控训练集损失/准确率。</strong><br>
我这段代码只在每个 epoch 后打印测试集准确率，跑久了有时候会过拟合都不知道。更好的做法是每个 epoch 结束时，也在训练集上算个 loss 或 accuracy 曲线，这样能直观地判断哪里需要减小学习率或者早停（Early Stopping）。</p>
</li>
<li>
<p><strong>数据清洗。</strong><br>
现在对文本只做了 <code>jieba</code> 分词，没有去停用词、标点符号、URL 等噪声。如果新闻里有链接、邮箱、HTML 标签，最好先正则去除，再分词效果会更干净。</p>
</li>
<li>
<p><strong>平衡数据与评价指标。</strong><br>
如果新闻类别分布不均衡，比如“娱乐”占了太多，而“科技”很少，仅仅看整体准确率容易误导。建议加上 <code>precision/recall/F1</code>，或者把某个特别关心的类别 ROC-AUC 单独看一下。</p>
</li>
<li>
<p><strong>超参数调优。</strong><br>
TextCNN 里用的滤波器数量是 100、嵌入维度是 300、截断长度 100，这些都可以调。根据新闻文本平均长度，可以把 <code>max_len</code> 调到 50、80 或者 200； <code>num_filters</code> 可以调成 200、300 看效果；卷积核大小也可以加上 <code>(2,3,4)</code>、<code>(3,4,5,6)</code> 组合试试。做网格搜索或贝叶斯优化能找到更优 hyper-parameters。</p>
</li>
</ol>
<hr>
<h2 id="版本二：multi-granularity-bert-增强版">版本二：Multi-Granularity BERT 增强版</h2>
<p>TextCNN 是个好基线，但它对长距离依赖和深层语义的理解有限。于是我在第二版里直接用 BERT 预训练模型，把它当成一个强大的特征提取器，然后在 BERT 的基础上再做多粒度融合、多尺度卷积和自注意力，尽可能兼顾局部模式和全局上下文。先贴出代码，再解释思路。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path, tokenizer, max_len=<span class="number">128</span>, label_map=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = tokenizer</span><br><span class="line">        <span class="variable language_">self</span>.max_len = max_len</span><br><span class="line">        <span class="variable language_">self</span>.texts, <span class="variable language_">self</span>.labels = <span class="variable language_">self</span>._load_data(file_path)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> label_map <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 训练集将标签去重排个序，测试集沿用</span></span><br><span class="line">            <span class="variable language_">self</span>.unique_labels = <span class="built_in">sorted</span>(<span class="built_in">set</span>(<span class="variable language_">self</span>.labels))</span><br><span class="line">            <span class="variable language_">self</span>.label_map = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.unique_labels)&#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.label_map = label_map</span><br><span class="line">            invalid_labels = [l <span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable language_">self</span>.labels <span class="keyword">if</span> l <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.label_map]</span><br><span class="line">            <span class="keyword">if</span> invalid_labels:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">f&quot;有无法对齐的标签: <span class="subst">&#123;<span class="built_in">set</span>(invalid_labels)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_data</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        texts, labels = [], []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                label, text = line.strip().split(<span class="string">&#x27;\t&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">                texts.append(text)</span><br><span class="line">                labels.append(label)</span><br><span class="line">        <span class="keyword">return</span> texts, labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.texts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        text = <span class="variable language_">self</span>.texts[idx]</span><br><span class="line">        encoding = <span class="variable language_">self</span>.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=<span class="variable language_">self</span>.max_len,</span><br><span class="line">            padding=<span class="string">&#x27;max_length&#x27;</span>,</span><br><span class="line">            truncation=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>: encoding[<span class="string">&#x27;input_ids&#x27;</span>].squeeze(<span class="number">0</span>),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>: encoding[<span class="string">&#x27;attention_mask&#x27;</span>].squeeze(<span class="number">0</span>).<span class="built_in">bool</span>(),</span><br><span class="line">            <span class="string">&#x27;label&#x27;</span>: torch.tensor(<span class="variable language_">self</span>.label_map[<span class="variable language_">self</span>.labels[idx]], dtype=torch.long)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiGranularityBERT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, pretrained_name=<span class="string">&#x27;bert-base-chinese&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 加载预训练 BERT</span></span><br><span class="line">        <span class="variable language_">self</span>.bert = AutoModel.from_pretrained(pretrained_name)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 多尺度卷积：in=768, out=256, kernel=3,5,7</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_layers = nn.ModuleList([</span><br><span class="line">            nn.Conv1d(<span class="number">768</span>, <span class="number">256</span>, kernel_size=k, padding=k//<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>]</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 多头自注意力：embed_dim=256*3=768, num_heads=8</span></span><br><span class="line">        <span class="variable language_">self</span>.attention = nn.MultiheadAttention(</span><br><span class="line">            embed_dim=<span class="number">256</span> * <span class="number">3</span>,</span><br><span class="line">            num_heads=<span class="number">8</span>,</span><br><span class="line">            batch_first=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分类头</span></span><br><span class="line">        <span class="variable language_">self</span>.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span> * <span class="number">3</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.3</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        <span class="comment"># BERT 前向，返回所有隐藏层</span></span><br><span class="line">        outputs = <span class="variable language_">self</span>.bert(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            output_hidden_states=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        hidden_states = outputs.hidden_states</span><br><span class="line">        <span class="comment"># hidden_states[0]: embedding 层</span></span><br><span class="line">        <span class="comment"># hidden_states[-4]: 倒数第4层</span></span><br><span class="line">        <span class="comment"># hidden_states[-1]: 顶层</span></span><br><span class="line">        <span class="comment"># 取三层做融合</span></span><br><span class="line">        conv_input = torch.stack([</span><br><span class="line">            hidden_states[-<span class="number">1</span>],</span><br><span class="line">            hidden_states[-<span class="number">4</span>],</span><br><span class="line">            hidden_states[<span class="number">0</span>]</span><br><span class="line">        ], dim=-<span class="number">1</span>).mean(dim=-<span class="number">1</span>)  <span class="comment"># [batch, seq_len, 768]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 多尺度卷积：先 permute -&gt; [batch, 768, seq_len]</span></span><br><span class="line">        conv_outputs = []</span><br><span class="line">        <span class="keyword">for</span> conv <span class="keyword">in</span> <span class="variable language_">self</span>.conv_layers:</span><br><span class="line">            x = conv_input.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)   <span class="comment"># [B, 768, L]</span></span><br><span class="line">            conv_out = conv(x)               <span class="comment"># [B, 256, L]</span></span><br><span class="line">            conv_out = nn.functional.gelu(conv_out)</span><br><span class="line">            conv_outputs.append(conv_out.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>))  <span class="comment"># [B, L, 256]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 拼接 -&gt; [B, L, 768]</span></span><br><span class="line">        combined = torch.cat(conv_outputs, dim=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 自注意力：mask 掉 padding</span></span><br><span class="line">        key_padding_mask = (attention_mask == <span class="number">0</span>)</span><br><span class="line">        attn_output, _ = <span class="variable language_">self</span>.attention(</span><br><span class="line">            combined, combined, combined,</span><br><span class="line">            key_padding_mask=key_padding_mask</span><br><span class="line">        )  <span class="comment"># [B, L, 768]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 平均池化 -&gt; [B, 768]</span></span><br><span class="line">        pooled = attn_output.mean(dim=<span class="number">1</span>)</span><br><span class="line">        logits = <span class="variable language_">self</span>.classifier(pooled)  <span class="comment"># [B, num_classes]</span></span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_enhanced</span>():</span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;bert-base-chinese&#x27;</span>)</span><br><span class="line">    train_set = TextDataset(<span class="string">&#x27;train.txt&#x27;</span>, tokenizer)</span><br><span class="line">    test_set = TextDataset(<span class="string">&#x27;test.txt&#x27;</span>, tokenizer, label_map=train_set.label_map)</span><br><span class="line">    </span><br><span class="line">    label_map = train_set.label_map</span><br><span class="line">    id2label = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> label_map.items()&#125;</span><br><span class="line">    </span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    model = MultiGranularityBERT(num_classes=<span class="built_in">len</span>(label_map))</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;使用 <span class="subst">&#123;torch.cuda.device_count()&#125;</span> 个 GPU 进行训练&quot;</span>)</span><br><span class="line">        model = nn.DataParallel(model)</span><br><span class="line">    model.to(device)</span><br><span class="line">    model_core = <span class="built_in">getattr</span>(model, <span class="string">&#x27;module&#x27;</span>, model)</span><br><span class="line"></span><br><span class="line">    optimizer = optim.AdamW([</span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: model_core.bert.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">2e-5</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;params&#x27;</span>: model_core.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">    ], weight_decay=<span class="number">0.01</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">    train_loader = DataLoader(train_set, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    test_loader = DataLoader(test_set, batch_size=<span class="number">64</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        epoch_bar = tqdm(train_loader, desc=<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/10&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> epoch_bar:</span><br><span class="line">            inputs = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items() <span class="keyword">if</span> k != <span class="string">&#x27;label&#x27;</span>&#125;</span><br><span class="line">            labels = batch[<span class="string">&#x27;label&#x27;</span>].to(device)</span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(**inputs)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            epoch_bar.set_postfix(&#123;<span class="string">&#x27;loss&#x27;</span>: <span class="string">f&quot;<span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>, <span class="string">&#x27;lr&#x27;</span>: optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 验证阶段</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        all_preds, all_labels = [], []</span><br><span class="line">        class_correct = &#123;cls: <span class="number">0</span> <span class="keyword">for</span> cls <span class="keyword">in</span> label_map.keys()&#125;</span><br><span class="line">        class_total = &#123;cls: <span class="number">0</span> <span class="keyword">for</span> cls <span class="keyword">in</span> label_map.keys()&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            test_bar = tqdm(test_loader, desc=<span class="string">&quot;测试中&quot;</span>, leave=<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> test_bar:</span><br><span class="line">                inputs = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items() <span class="keyword">if</span> k != <span class="string">&#x27;label&#x27;</span>&#125;</span><br><span class="line">                labels = batch[<span class="string">&#x27;label&#x27;</span>].to(device)</span><br><span class="line">                </span><br><span class="line">                outputs = model(**inputs)</span><br><span class="line">                preds = torch.argmax(outputs, dim=<span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">                all_preds.extend(preds.cpu().numpy())</span><br><span class="line">                all_labels.extend(labels.cpu().numpy())</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> label, pred <span class="keyword">in</span> <span class="built_in">zip</span>(labels, preds):</span><br><span class="line">                    cls_name = id2label[label.item()]</span><br><span class="line">                    class_total[cls_name] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> pred == label:</span><br><span class="line">                        class_correct[cls_name] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">                acc_so_far = <span class="number">100</span> * <span class="built_in">sum</span>(class_correct.values()) / <span class="built_in">sum</span>(class_total.values())</span><br><span class="line">                test_bar.set_postfix(&#123;<span class="string">&#x27;acc&#x27;</span>: <span class="string">f&quot;<span class="subst">&#123;acc_so_far:<span class="number">.1</span>f&#125;</span>%&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印分类报告</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span> + <span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> 分类报告:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(classification_report(all_labels, all_preds, target_names=label_map.keys(), digits=<span class="number">4</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\n各类别准确率:&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> cls <span class="keyword">in</span> label_map:</span><br><span class="line">            total = class_total[cls]</span><br><span class="line">            correct = class_correct[cls]</span><br><span class="line">            acc = <span class="number">100</span> * correct / total <span class="keyword">if</span> total &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;  <span class="subst">&#123;cls&#125;</span>: <span class="subst">&#123;acc:<span class="number">.2</span>f&#125;</span>% (<span class="subst">&#123;correct&#125;</span>/<span class="subst">&#123;total&#125;</span>)&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存最佳模型</span></span><br><span class="line">        current_acc = <span class="built_in">sum</span>(class_correct.values()) / <span class="built_in">sum</span>(class_total.values())</span><br><span class="line">        <span class="keyword">if</span> current_acc &gt; best_acc:</span><br><span class="line">            best_acc = current_acc</span><br><span class="line">            torch.save(<span class="built_in">getattr</span>(model, <span class="string">&#x27;module&#x27;</span>, model).state_dict(), <span class="string">&#x27;best_model.pth&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;发现新最佳模型，准确率: <span class="subst">&#123;best_acc:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_enhanced()</span><br></pre></td></tr></table></figure>
<p>这段代码里，我直接用了 <code>bert-base-chinese</code>，并且在 <code>forward</code> 中把 <code>output_hidden_states=True</code>，拿到 BERT 的 13 层输出 (<code>hidden_states</code>)。为了把不同层次的语义信息都用上，选取最顶层（第 12 层）、倒数第 4 层（第 8 层）和最底层 embedding（第 0 层），把它们在“隐藏向量维度”上堆叠并求平均，得到一个融合后的向量 <code>conv_input</code>，形状是 <code>[batch, seq_len, 768]</code>。</p>
<p>接着，做三路 Conv1d，窗口大小分别是 3、5、7，对 <code>conv_input</code> 先做维度变换变成 <code>[batch, 768, seq_len]</code>，然后经过 <code>nn.Conv1d(768,256,kernel=k,padding=k//2)</code>，得到 <code>[batch,256,seq_len]</code>，激活用 GELU，然后再把维度变回 <code>[batch,seq_len,256]</code>，三路拼接成 <code>[batch,seq_len,768]</code>。这一步可以看作是在“词向量+位置信息+中层语义+高层语义”融合之后，进行不同粒度的 n-gram 局部特征提取。</p>
<p>卷积之后，我加了一个多头自注意力层 (<code>nn.MultiheadAttention(embed_dim=768,num_heads=8,batch_first=True)</code>)。它的输入、键、值都用同一个 <code>combined</code>，并且用 <code>key_padding_mask=(attention_mask==0)</code> 来屏蔽掉 padding。这样每个 token 的新表示就不仅有局部卷积特征，还融合了全局上下文，输出形状仍然是 <code>[batch,seq_len,768]</code>。</p>
<p>最后对那条序列在时间维度上做平均池化（<code>pooled = attn_output.mean(dim=1)</code>），得到 <code>[batch,768]</code>，再串联两层全连接（512 + ReLU + Dropout + 最终映射到类别数）拿到 logits，送交叉熵损失训练。同 TextCNN 那里类似，我在训练循环里也做了分层学习率：BERT 参数 lr=2e-5，分类头 lr=1e-3，梯度裁剪 norm_max=1.0，多 GPU 自动并行，测试阶段还打印分类报告和各类别准确率，并保存最优模型。</p>
<h3 id="版本一-vs-版本二：改进思路">版本一 vs 版本二：改进思路</h3>
<p>第一版跑通之后，整体结构和流程很清晰了，但是体验到模型在理解语义时有明显局限：TextCNN 只能捕捉固定窗口大小的 n-gram（比如说“人民大会堂”这种连续三词组合能抓到，但是一句话里后半截的上下文就没办法感知到），如果新闻比较长、句子间需要远距离依赖，就很难分辨。有好几次我试过把 <code>max_len</code> 拉到 200，发现效果也不明显提升，反而训练慢。</p>
<p>所以第二版就上了 BERT，把它当作一棵已经“训练好”的深度语义特征树，用它提取的上下文向量再去做更深层次的加工。具体改进点如下：</p>
<ol>
<li>
<p><strong>分词和词表</strong>：<br>
TextCNN 里用 <code>jieba</code> 分词，还得自己去统计高频 20000 词做词表。但 BERT 自带的分词器（WordPiece/BPE）已经比较完备，词表在几十万规模，拆子词也能更好地涵盖新词和低频词。这样我们不再担心 OOV（out-of-vocabulary）的问题，也不用自己统计词频、构造词表。</p>
</li>
<li>
<p><strong>多层级特征</strong>：<br>
BERT 每层 encoder 都捕捉了不同层次的语义，底层 embedding 偏向词形+位置，中层更偏句法结构，顶层偏全局上下文。直接用顶层有时会丢一些低层信息，比如词性和近似词义；只用底层则没有全局视野。融合这三层能让模型对“局部搭配”和“全局语义”都有感知。</p>
</li>
<li>
<p><strong>多尺度卷积 + 自注意力</strong>：<br>
TextCNN 本身有多尺度卷积，但它从头到尾都是在随机初始化的嵌入上做卷积。第二版先用 BERT 抽取出高维向量序列，再在高维向量序列上继续做卷积，能得到更具语义信息的 n-gram 特征。同时加一个自注意力层，能让模型在局部卷积特征基础上再捕获跨越很远的依赖（比如标题和结尾的呼应），提高整体表示能力。</p>
</li>
<li>
<p><strong>分层学习率与细粒度调参</strong>：<br>
TextCNN 用同一个学习率就行；BERT 预训练参数如果不小心用太大 lr，很容易把预训练权重“冲散”，只要 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> 级别就好。新加的卷积层和分类头一开始是随机初始化，需要稍大点的 lr 才能快速收敛，用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span> 刚好。这样分层调参，有助于整体更稳定地训练。</p>
</li>
</ol>
<p>综上，第二版在训练速度、效果和稳定性上都比第一版有明显提升，但也要付出更多硬件资源和时间成本：最少得用一块 12GB 以上显存的 GPU，如果数据集大可能要多卡并行。如果只是想快速搭建一个 baseline，第一版就够用了。如果想追求更高准确率或产品化，第二版更适合。</p>
<hr>
<h2 id="总结">总结</h2>
<ol>
<li><strong>版本一</strong> 用 TextCNN：流程简单，自己做分词、词表、固定长度索引，跑得快，代码量少，最适合作为入门或资源有限时的 baseline。</li>
<li><strong>版本二</strong> 用 BERT + 多粒度卷积 + 自注意力：利用预训练模型的语义能力，再叠加卷积和注意力，能更好地抓住新闻里的深层次信息，适合追求更高准确率的场景，但需要更多 GPU 资源和更细腻的调参。</li>
</ol>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://xxxcoolestfish.github.io/2025/03/28/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE%EF%BC%9A%E4%BB%8ETextCNN%E5%88%B0%E5%A4%9A%E7%B2%92%E5%BA%A6BERT%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%BC%94%E8%BF%9B/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/" rel="tag">BERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/" rel="tag">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Self-Attention/" rel="tag">Self-Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TextCNN/" rel="tag">TextCNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%9A%E7%B2%92%E5%BA%A6%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/" rel="tag">多粒度特征融合</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2025/05/06/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E8%B6%8B%E5%8A%BF%E9%A2%84%E6%B5%8B%E7%9A%84%E5%81%87%E6%96%B0%E9%97%BB%E6%A3%80%E6%B5%8B%E6%B3%9B%E5%8C%96%E6%A1%86%E6%9E%B6/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            论文精读：基于时间趋势预测的假新闻检测泛化框架
          
        </div>
      </a>
    
    
      <a href="/2025/03/20/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%E4%BA%BA%E8%84%B8%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%90%86%E5%88%B061-%E5%87%86%E7%A1%AE%E7%8E%87%E7%9A%84%E5%AE%9E%E6%88%98%E7%AA%81%E7%A0%B4/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">深入解析人脸表情识别：从原理到61%准确率的实战突破</div>
      </a>
    
  </nav>

  
   
  
   
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2025
        <i class="ri-heart-fill heart_icon"></i> Xinyu Zhuang
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/logo.svg" alt="很荣幸被您认识"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->
 
    
        <link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.15.1/katex.min.css">
        <script src="https://cdn.staticfile.org/KaTeX/0.15.1/katex.min.js"></script>
        <script src="https://cdn.staticfile.org/KaTeX/0.15.1/contrib/auto-render.min.js"></script>
        
    
 
<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>